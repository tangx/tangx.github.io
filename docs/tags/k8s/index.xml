<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>k8s on 老麦的书房</title><link>https://typonotes.com/tags/k8s/</link><description>Recent content in k8s on 老麦的书房</description><generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Tue, 27 Jun 2023 10:35:12 +0800</lastBuildDate><atom:link href="https://typonotes.com/tags/k8s/index.xml" rel="self" type="application/rss+xml"/><item><title>《容器云平台排错一览图》</title><link>https://typonotes.com/posts/2023/06/27/cloudnative-k8s-debug-flow/</link><pubDate>Tue, 27 Jun 2023 10:35:12 +0800</pubDate><guid>https://typonotes.com/posts/2023/06/27/cloudnative-k8s-debug-flow/</guid><description>《容器云平台排错一览图》 建议点击 查看原文 查看最新内容。 原文链接: https://typonotes.com/posts/2023/06/27/cloudnative-k8s-debug-flow/ 这张 《容器云平台排错一览图》 不仅 逻辑清晰的为我们提供了 排错思路、流程 和 check list。 而且 通过不同颜色， 非常贴心的为我们提供常用建议。 黑色： mingling 绿色： 直接修复方案 蓝色： 建议 感谢作者。 图片来源： learnk8s.io 绘制/勘误： Spark 点击下载 高清</description></item><item><title>Aliyun Logtail 收集 JSON 格式日志</title><link>https://typonotes.com/posts/2023/05/08/aliyun-logtail-collect-json-format-logs/</link><pubDate>Mon, 08 May 2023 16:54:22 +0800</pubDate><guid>https://typonotes.com/posts/2023/05/08/aliyun-logtail-collect-json-format-logs/</guid><description>Aliyun Logtail 收集 JSON 格式日志 如果在 公众号 文章发现状态为 已更新， 建议点击 查看原文 查看最新内容。 状态: 未更新 原文链接: https://typonotes.com/posts/2023/05/08/aliyun-logtail-collect-json-format-logs/ 本文针对性比较强， 仅仅适用于 Aliyun Logtail 配置。 在使用 Aliyun K8S 集群后， 可以安装 Logtail 服务进行日志收集。 默认文档中， 阿里云官方提供的是 正则模式 的采集方式 采集 Nginx Ingress Controller 日志 。 将日志改成 JSON 格式之后， 可以</description></item><item><title>Nginx 和 Nginx-Ingress-Controller 配置 JSON 日志格式</title><link>https://typonotes.com/posts/2023/05/08/nginx-log-json-format/</link><pubDate>Mon, 08 May 2023 09:34:49 +0800</pubDate><guid>https://typonotes.com/posts/2023/05/08/nginx-log-json-format/</guid><description>Nginx 和 Nginx-Ingress-Controller 配置 JSON 日志格式 如果在 公众号 文章发现状态为 已更新， 建议点击 查看原文 查看最新内容。 状态: 未更新 原文链接: https://typonotes.com/posts/2023/05/08/nginx-log-format/ Nginx 注意: 列表中的字段仅仅是 Demo ，根据自己实际需求进行增删。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 { &amp;#34;time&amp;#34;: &amp;#34;$time_iso8601&amp;#34;, &amp;#34;remote_addr&amp;#34;: &amp;#34;$remote_addr&amp;#34;, &amp;#34;remote_user&amp;#34;: &amp;#34;$remote_user&amp;#34;, &amp;#34;request_method&amp;#34;: &amp;#34;$request_method&amp;#34;, &amp;#34;host&amp;#34;: &amp;#34;$host&amp;#34;, &amp;#34;request_uri&amp;#34;: &amp;#34;$request_uri&amp;#34;, &amp;#34;status&amp;#34;: &amp;#34;$status&amp;#34;, &amp;#34;request_time&amp;#34;: &amp;#34;$request_time&amp;#34;, &amp;#34;cost&amp;#34;: &amp;#34;$request_time&amp;#34;, &amp;#34;body_bytes_sent&amp;#34;: &amp;#34;$body_bytes_sent&amp;#34;, &amp;#34;http_referer&amp;#34;: &amp;#34;$http_referer&amp;#34;, &amp;#34;http_user_agent&amp;#34;: &amp;#34;$http_user_agent&amp;#34;, &amp;#34;request_length&amp;#34;: &amp;#34;$request_length&amp;#34;, &amp;#34;upstream_addr&amp;#34;: &amp;#34;$upstream_addr&amp;#34;, &amp;#34;upstream_response_length&amp;#34;: &amp;#34;$upstream_response_length&amp;#34;, &amp;#34;upstream_response_time&amp;#34;: &amp;#34;$upstream_response_time&amp;#34;, &amp;#34;upstream_status&amp;#34;: &amp;#34;$upstream_status&amp;#34;, &amp;#34;opentelemetry_context_traceparent&amp;#34;:&amp;#34;$opentelemetry_context_traceparent&amp;#34;,</description></item><item><title>开发 k8s 管理平台 - k8sailor 08. 获取并展示 Deployments 信息</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/08-fetch-and-display-deployments/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/08-fetch-and-display-deployments/</guid><description>开发 k8s 管理平台 - k8sailor 08. 获取并展示 Deployments 信息 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/08-fetch-and-display-deployments/ tag: https://github.com/tangx/k8sailor/tree/feat/08-fetch-and-display-deployments 使用 Axios 请求 Deployments 数据 安装 axios 客户端 1 2 # 安装 axios yarn add axios 创建 /webapp/src/apis 目录， 用于存放所有针对 k8sailor 后端的数据请求 使用 axios config 模式初始化一个客户端 /webapp/src/apis/httpc.ts axios config 模式可以创建一个 http 客户端，其中包含了各种各样的初始化参数， 使用这个模式就不用在每个请求中都写重复的内容了</description></item><item><title>开发 k8s 管理平台 - k8sailor 09. 通过 deployment label 获取 pod 信息</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/09-get-pods-by-deployment-label/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/09-get-pods-by-deployment-label/</guid><description>开发 k8s 管理平台 - k8sailor 09. 通过 deployment label 获取 pod 信息 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/09-get-pods-by-deployment-label/ tag: https://github.com/tangx/k8sailor/tree/feat/09-get-pods-by-deployment-label 有了之前结构铺垫， 获取 Pod 还是很简单简单的。 其中需要注意的是 ListOptions 中的 LabelSelector 是一个字符串， 多组 key=value 之间使用 逗号 , 进行连接。 1 labelSelector := `key1=value1,key2=value2,...` 而通过 client-go API 获取的 Deployment, Pod 等信息中的 MatchLabel 字段是一个 map[string]string 的 map。 因此， 在使用 k8s client 查询的时候， 需要对进行一些传参转换。 1</description></item><item><title>开发 k8s 管理平台 - k8sailor 11. 展示 deployment 详情页</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/11-display-deployment-detail/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/11-display-deployment-detail/</guid><description>开发 k8s 管理平台 - k8sailor 11. 展示 deployment 详情页 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/11-display-deployment-detail/ tag: https://github.com/tangx/k8sailor/tree/feat/11-display-deployment-detail 之前在后端已经将详情页的展示接口拆成了 2个 其一是根据 name 获取 单个 deployment /deployments/:name 其二是根据 deployment name 获取 关联 的 pods 信息 /deployments/:name/pod 页面展示就是两个接口请求与数据展示的简单操作， 和之前 deployment 页面一样， 没什么好说的。 typescript 的 interface 衍生 不过， 在遇到第二个、第三个接口出现的时候， 发现</description></item><item><title>开发 k8s 管理平台 - k8sailor 12. 设置 deployment 副本数量 与 参数的有效性验证</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/12-deployment-scale-and-params-validate/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/12-deployment-scale-and-params-validate/</guid><description>开发 k8s 管理平台 - k8sailor 12. 设置 deployment 副本数量 与 参数的有效性验证 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/12-deployment-scale-and-params-validate/ tag: https://github.com/tangx/k8sailor/tree/feat/12-deployment-scale-and-params-validate deployment scale 1 kubectl scale deployment my-nginx-1 --replicas 1 在 client-go sdk 中， scale 参数是一个对象， 因此不能直接传入 一个数字。 需要通过 GetScale() 方法获取到 *autoscalingv1.Scale 对象。 修改 Scale 对象中的 Replicas 数值。 使用 UpdateScale() 方法更新设置。 SetDeploymentReplicas params validtor 参数验证在任何情况下都不能放松警惕， 尤其是 边界验证 和 0值混淆 。 对</description></item><item><title>开发 k8s 管理平台 - k8sailor 13. 使用 k8s informer 订阅集群事件</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/13-k8s-informer/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/13-k8s-informer/</guid><description>开发 k8s 管理平台 - k8sailor 13. 使用 k8s informer 订阅集群事件 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/13-k8s-informer/ tag: https://github.com/tangx/k8sailor/tree/feat/13-k8s-informer 从应用层面来说， 创建 informer 并启动之后就与 k8s cluster 创建了一个长链接并订阅了 某个资源 Resource 的变化。 至于订阅后得到的数据要怎么用完全取决于订阅者的业务设计。 Shared Informer Factory 共享机制 Informer 又称为 Shared Informer，表明是可以共享使用的，在使用 client-go 写代码时，若同</description></item><item><title>开发 k8s 管理平台 - k8sailor 14. 一些前后端代码优化</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/14-some-optimize/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/14-some-optimize/</guid><description>开发 k8s 管理平台 - k8sailor 14. 一些前后端代码优化 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/14-some-optimize/ tag: https://github.com/tangx/k8sailor/tree/feat/14-some-optimize 将 LabelSelector 转换为 Selector client-go 提供了一个方法， 可以将 Resource 中的 LabelSelector 转换为 Selector, 并且 Selector 结构提供了一些常用的方法。 如 String 1 2 3 4 5 6 7 8 9 import ( metav1 &amp;#34;k8s.io/apimachinery/pkg/apis/meta/v1&amp;#34; ) func() { selector, _ := metav1.LabelSelectorAsSelector(dep.Spec.Selector) x := selector.String() fmt.Println(x) }() 因此在使用 GetXXXByLabels 时， api 层 可以考虑 接收 map[string]string 类型的参数。 而在 biz 层应该将 不同类型 的参数 统一 转换为格式</description></item><item><title>开发 k8s 管理平台 - k8sailor 15. 根据名字删除 deployment 和 pod</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/15-delete-deployment-and-pod-by-name/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/15-delete-deployment-and-pod-by-name/</guid><description>开发 k8s 管理平台 - k8sailor 15. 根据名字删除 deployment 和 pod 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/15-delete-deployment-and-pod-by-name/ tag: https://github.com/tangx/k8sailor/tree/feat/15-delete-deployment-and-pod-by-name 调用 k8s api 没什么好说的。 k8sdao 1 2 3 4 5 func DeleteDeploymentByName(ctx context.Context, namespace string, name string) error { opts := metav1.DeleteOptions{} return clientset.AppsV1().Deployments(namespace).Delete(ctx, name, opts) } biz 1 2 3 4 5 6 7 8 9 10 11 12 13 14 type DeleteDeploymentByNameInput struct { Name string `uri:&amp;#34;name&amp;#34;` Namespace string `query:&amp;#34;namespace&amp;#34;` } // DeleteDeploymentByName 根据名字删除 deployment func DeleteDeploymentByName(ctx context.Context, input DeleteDeploymentByNameInput) error { err := k8sdao.DeleteDeploymentByName(ctx, input.Namespace, input.Name) if err != nil { return fmt.Errorf(&amp;#34;k8s internal error: %w&amp;#34;, err) } return nil } api 1 2 3 4 5 6 7 8 9 10 11 12 13 14 func handlerDeleteDeploymentByName(c</description></item><item><title>开发 k8s 管理平台 - k8sailor 16. 创建 Deployment</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/16-create-deployment/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/16-create-deployment/</guid><description>开发 k8s 管理平台 - k8sailor 16. 创建 Deployment 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/16-create-deployment/ tag: https://github.com/tangx/k8sailor/tree/feat/16-create-deployment 使用 kubectl 命令创建如下 1 kubectl create deployment my-nginx-5 --image=nginx:alpine --replicas=3 --port=80 创建成功后查看结果， 大部分参数为默认参数。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 # kgd -o yaml my-nginx-5 apiVersion: apps/v1 kind: Deployment metadata: labels: app: my-nginx-5 # 根据 deployment 自动匹配名字自动生成 name: my-nginx-5 # 用户指定 namespace: default # 用户选择，默认为当前 namespace spec: progressDeadlineSeconds: 600 # 默认</description></item><item><title>开发 k8s 管理平台 - k8sailor 17. Pod 的阶段(phase)与状态(status)</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/17-pod-phase-and-status/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/17-pod-phase-and-status/</guid><description>开发 k8s 管理平台 - k8sailor 17. Pod 的阶段(phase)与状态(status) 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/17-pod-phase-and-status/ Pod 的生命周期 https://kubernetes.io/zh/docs/concepts/workloads/pods/pod-lifecycle/ Pod 的 Status 不是 Phase。 Pod 的 Status 需要根据 Pod 中的 ContainerStatuses 进行计算得到。 Phase 阶段 描述 Pending（悬决） Pod 已被 Kubernetes 系统接受，但有一个或者多个容器尚未创建亦未运行。此阶段包括等待 Pod 被调度的时间和通过网络下载镜</description></item><item><title>开发 k8s 管理平台 - k8sailor 19. 为 Deployment 创建 Service</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/19-create-service/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/19-create-service/</guid><description>开发 k8s 管理平台 - k8sailor 19. 为 Deployment 创建 Service 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/19-create-service/ tag: https://github.com/tangx/k8sailor/tree/feat/19-create-service https://kubernetes.io/zh/docs/concepts/services-networking/service/#externalname 1 2 3 kubectl create service clusterip nginx-web --clusterip=&amp;#34;port:targetPort&amp;#34; kubectl create service clusterip nginx-web --clusterip=&amp;#34;8082:80&amp;#34; kubectl create service nodeport nginx-web --clusterip=&amp;#34;8081:80&amp;#34; 需要注意, 使用 kubectl get service 查看到的 Ports 的展示结果为 port:nodePort， 而 targetPort 不展示。 1 2 3 # kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE demo-nginx-nodeport-3 NodePort 10.43.181.29 &amp;lt;none&amp;gt; 80:32425/TCP 4s port, targetPort, nodePort 端口映射中的四个 比较关键 的要素: name: 避免端口相同时，默认名字冲突 port:</description></item><item><title>开发 k8s 管理平台 - k8sailor 20. 为 Deployment 创建 Ingress</title><link>https://typonotes.com/posts/books/k8sailor/chapter02/20-create-ingress/</link><pubDate>Tue, 10 Jan 2023 10:28:03 +0800</pubDate><guid>https://typonotes.com/posts/books/k8sailor/chapter02/20-create-ingress/</guid><description>开发 k8s 管理平台 - k8sailor 20. 为 Deployment 创建 Ingress 原文地址: https://tangx.in/posts/books/k8sailor/chapter02/01-install-k3s-cluster/ tag: https://github.com/tangx/k8sailor/tree/feat/20-create-ingress k8s ingress https://kubernetes.io/zh/docs/concepts/services-networking/ingress/ 1 2 3 4 # Create an ingress with a default backend kubectl create ingress ingdefault --class=default \ --default-backend=defaultsvc:http \ --rule=&amp;#34;foo.com/*=svc:8080,tls=secret1&amp;#34; --dry-run -o yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: networking.k8s.io/v1 kind: Ingress metadata: creationTimestamp: null name: ingdefault spec: defaultBackend: service: name: defaultsvc port: name: http ingressClassName: default rules: - host: foo.com http: paths: - backend: service: name: svc port: number: 8080 path: / pathType: Prefix # 匹配方式 tls: - hosts: - foo.com secretName: secret1 status: loadBalancer: {} 路径类型 Ingress 中的每个</description></item><item><title>K8S 中被挂载的 Configmap 发生了变化容器内部会发生什么</title><link>https://typonotes.com/posts/2021/12/02/configmap-mounting-scenario-when-updated/</link><pubDate>Thu, 02 Dec 2021 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2021/12/02/configmap-mounting-scenario-when-updated/</guid><description>K8S 中被挂载的 Configmap 发生了变化容器内部会发生什么 1. 使用 env 挂载 被挂载的值不会变 1 2 3 4 5 6 7 env: # 定义环境变量 - name: PLAYER_INITIAL_LIVES # 请注意这里和 ConfigMap 中的键名是不一样的 valueFrom: configMapKeyRef: name: game-demo # 这个值来自 ConfigMap key: player_initial_lives # 需要取值的键 使用 volumeMounts 挂载目录 在使用 volumeMounts 挂载的时候， 根据是否有 subpath 参数， 情况也不一样。 2.1 没有 subpath 挂载目录 1 2 3 volumeMounts: - name: config mountPath: &amp;#34;/config/normal-dir/some-path/&amp;#34;</description></item><item><title>K8S 使用 TTL 控制器自动清理完成的 job pod</title><link>https://typonotes.com/posts/2021/09/23/k8s-ttl-seconds-after-finished-forbidden/</link><pubDate>Thu, 23 Sep 2021 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2021/09/23/k8s-ttl-seconds-after-finished-forbidden/</guid><description>K8S 使用 TTL 控制器自动清理完成的 Job Pod 最近为集群 CI 默认添加了 .spec.ttlSecondsAfterFinished 参数， 以便在 cronjob 和 job 运行完成后自动清理过期 pod 。 但是在 CI 的时候却失败， 报错如下。 1 spec.jobTemplate.spec.ttlSecondsAfterFinished: Forbidden: disabled by feature-gate 核查资料得知， 在 v1.21 之前， 该开关默认是关闭的。 刚好错误集群低于此版本。 Job TTL 控制器 K8S 提供了一个 TTL 控制器， 可以自动在 JOB Complete 或 Failed 之后， 经过一定时间</description></item><item><title>5分钟k3s-什么是 K3s? K3s 简介与适用场景介绍</title><link>https://typonotes.com/posts/2021/06/05/k3s-introduce/</link><pubDate>Sat, 05 Jun 2021 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2021/06/05/k3s-introduce/</guid><description>什么是 K3s? K3s 是一个轻量级的 Kubernetes 发行版，它针对边缘计算、物联网等场景进行了高度优化。 K3s 有以下增强功能： 打包为单个二进制文件。 使用基于 sqlite3 的轻量级存储后端作为默认存储机制。同时支持使用 etcd3、MySQL 和 + PostgreSQL 作为存储机制。 封装在简单的启动程序中，通过该启动程序处理很多复杂的 TLS 和选项。 默</description></item><item><title>CronJob 和 Job 的 退出 POD 数量管理</title><link>https://typonotes.com/posts/2021/01/22/exited-pod-limits/</link><pubDate>Fri, 22 Jan 2021 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2021/01/22/exited-pod-limits/</guid><description>CronJob 和 Job 的 Pod 退出保留时间 cronjob 可以认为 CronJob 作为定时调度器， 在正确的时间创建 Job Pod 完成任务。 在 CronJob 中， 默认 .spec.successfulJobsHistoryLimit: 保留 3 个正常退出的 Job .spec.failedJobsHistoryLimit: 1 个异常退出的 Job 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: batch/v1beta1 kind: CronJob metadata: name: zeus-cron-checkqueue namespace: zeus-dev spec: schedule: &amp;#34;*/10 * * * *&amp;#34; failedJobsHistoryLimit: 1 successfulJobsHistoryLimit: 3 jobTemplate: spec: template: # ... 略 https://github.com/kubernetes/kubernetes/issues/64056 job 除了 cronjob 管理 job 之外， job 本身也提供 .spec.ttlSecondsAfterFinished 进行退出管理。 默认情况下 如果 ttlSecondsAfterFinished 值未</description></item><item><title>k8s 部署工具 kustomize 的实用小技巧</title><link>https://typonotes.com/posts/2020/12/05/kusutomize-usage-tips/</link><pubDate>Sat, 05 Dec 2020 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2020/12/05/kusutomize-usage-tips/</guid><description>k8s 部署工具 kustomize 的实用小技巧 在 k8s 上的部署， 大多组件都默认提供 helm 方式。 在实际使用中， 常常需要针对不通环境进行差异化配置。 个人觉得， 使用 kustomize 替换在使用和管理上，比直接使用 helm 参数更为清晰 。 同时组件在一个大版本下的部署方式通常不会有太大的变化， 没有必要重新维护一套部署文档，其实也不一定有精力这</description></item><item><title>calico 配置 BGP Route Reflectors</title><link>https://typonotes.com/posts/2019/12/10/calico-bgp-rr/</link><pubDate>Tue, 10 Dec 2019 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2019/12/10/calico-bgp-rr/</guid><description>calico 配置 BGP Route Reflectors Calico作为k8s的一个流行网络插件，它依赖BGP路由协议实现集群节点上的POD路由互通；而路由互通的前提是节点间建立 BGP Peer 连接。BGP 路由反射器（Route Reflectors，简称 RR）可以简化集群BGP Peer的连接方式，它是解决BGP扩展性问题的有效方式；具</description></item><item><title>calico 网络模型的简单笔记</title><link>https://typonotes.com/posts/2019/11/26/calico-simple-note/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2019/11/26/calico-simple-note/</guid><description>calico 简单笔记 calico 是一种基础 vRouter 的3层网络模型 (BGP 网络)。 在应用到 k8s 中，可以提到常见的 flannel。 使用节点主机作为 vRouter 实现 3层转发。 提高网络性能。 calico 的网络模型 calico 可以通过设置 IP-in-IP 控制网络模型: https://docs.projectcalico.org/v3.5/usage/configuration/ip-in-ip ipipMode=Never: BGP 模型。 完全不使用 IP-in-IP 隧道， 这就是常用的 BGP 模型。 ipipMode=Always: calico 节点直接通过 IP 隧道的的方式实现节点互通。 这实际</description></item><item><title>k8s nginx ingress 添加 x-forwarded</title><link>https://typonotes.com/posts/2019/08/10/nginx-ingress-x-forward/</link><pubDate>Sat, 10 Aug 2019 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2019/08/10/nginx-ingress-x-forward/</guid><description>ingress 配置 for-forward-for The client IP address will be set based on the use of PROXY protocol or from the X-Forwarded-For header value when use-forwarded-headers is enabled. https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#use-forwarded-headers https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/configmap/#forwarded-for-header 1 2 3 4 5 6 7 8 apiVersion: extensions/v1beta1 kind: Ingress metadata: name: srv-bff-op-center annotations: nginx.ingress.kubernetes.io/forwarded-for-header: &amp;#34;X-Forwarded-For&amp;#34; kubernetes.io/ingress.class: &amp;#34;nginx&amp;#34;</description></item><item><title>K8S 中使用 Heketi 管理 GlusterFS</title><link>https://typonotes.com/posts/2018/11/15/k8s-with-heketi/</link><pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2018/11/15/k8s-with-heketi/</guid><description>K8S 中使用 Heketi 管理 GlusterFS 与 官方文档不同 ， 本文中的 glusterfs 是独立与 k8s 之外的。 Heketi heketi 项目 为 GlusterFS 提供 RESTful 的 API 管理。 Requirements System must have glusterd service enabled and glusterfs-server installed Disks registered with Heketi must be in raw format. 目前提供两种管理方式: ssh, kubernetes heketi-ssh SSH Access SSH user and public key already setup on the node SSH user must have password-less sudo Must be able to run sudo commands from ssh. This requires disabling requiretty in the /etc/sudoers file 使用容器部署 https://hub.docker.com/r/heketi/heketi/ heketi-kubernetes 带实现 勘误 在使用 K8S 部署时， 如果客户端报错</description></item><item><title>K8S节点下载 gcr.io 原生镜像</title><link>https://typonotes.com/posts/2018/11/09/k8s-pull-image-from-gcr.io/</link><pubDate>Fri, 09 Nov 2018 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2018/11/09/k8s-pull-image-from-gcr.io/</guid><description>K8S下载 gcr.io 原生镜像 在国内是不能直接下载 gcr.io / k8s.gcr.io 等原生镜像的。 使用比较权威的三方源 aliyun , qcloud 将 gcr.io push 到 hub.docker.com 自建镜像代理 域名翻墙 域名翻墙 通过域名劫持，将目标地址直接解析到代理服务器上。 sniproxy 所有你需要的， 一个能直接访问 gcr.ip 的 https(443) 代理。 通过 sniproxy 实现。 通过 防火墙 , 安全组 限制访问来源。 1 2 # docker run -d --rm --network host --name sniproxy</description></item><item><title>k8s node 节点</title><link>https://typonotes.com/posts/2018/10/13/k8s-resource-node.md/</link><pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2018/10/13/k8s-resource-node.md/</guid><description>k8s node 节点介绍 node 是 k8s 的工作节点， cpu, memory 的提供者。 上面运行这实际工作的 pod。 node 的服务包括 container 环境、 kubelet 和 kube-proxy。 使用 kubectl 管理 node 基础语法为 : kubectl flag node &amp;lt;node_name&amp;gt; kubectl cordon / uncordon 1 2 3 4 # 驱逐 kubectl cordon node &amp;lt;node_name&amp;gt; # 恢复 kubectl uncordon node &amp;lt;node_name&amp;gt;</description></item><item><title>kubernetes POD 介绍</title><link>https://typonotes.com/posts/2018/10/13/k8s-object-pod.md/</link><pubDate>Sat, 13 Oct 2018 00:00:00 +0000</pubDate><guid>https://typonotes.com/posts/2018/10/13/k8s-object-pod.md/</guid><description>k8s POD 介绍 POD 在 k8s 中是最小管理单位。</description></item></channel></rss>